# Morning Paper Challenge

+ 文丞相：“孔曰成仁，孟曰取义，惟其义尽，所以仁至。读圣贤书，所学何事？而今而后，庶几无愧。”
+ 张子：“为天地立心，为生民立命，为往圣继绝学，为万世开太平。”
+ 阳明先生：“当读书做圣人耳。”

## Fifth Morning Paper Challenge

_One Hour, One Paper, Every Morning at 8am, At [here](https://www.google.ch/maps/place/%22Monte+Diggelmann%22+-+vantage+point+in+Irchelpark/@47.3933675,8.5491733,118m/data=!3m1!1e3!4m5!3m4!1s0x0:0x2cb79f95aa652fc3!8m2!3d47.3932358!4d8.5495728?hl=en)._

![Progress](https://progress-bar.dev/10/?scale=36&title=MPC&width=360&suffix=)

__2020-09-14__: [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf) :tada: This paper introduces temperature scaling in softmax for confidence calibration. Confidence calibration is an important issue that also ties to interpretability in areas such as metric learning. Essentially, tuning temperature manipulates the output entropy. One interesting finding is that the confidence calibration is actually low-dimensional, this is a great news because the generalization is then less affected by the over-confidence problem.

__2020-09-16__: [What is being transferred in transfer learning?](https://arxiv.org/pdf/2008.11687.pdf) :tada: This paper is very inspiring. By investigating the question given in the title, the author offered many insights. The insights are particularly useful for related domain such as low-shot learning and continual learning. The design of the experiments are simply genius. I also learned a cool feature similarity method called CKA (centered kernel alignment).

__2020-09-17__: [Surrogate Gradient Learning in Spiking Neural Networks](https://arxiv.org/pdf/1901.09948.pdf) :tada: this is a long overdue, but I finally read it. The article is very easy to follow and I found it very enjoyable to read. Before reading this article, my preception of this work is a new paradiam of training multi-layer SNNs. What the article actually does is extending SuperSpike for implementing local errors at each layer and the authors did a brilliant overview of unifying all gradient-inspired approaches for training SNNs. I shall see what I can learn from the tutorials. 

__2020-09-18__: [A tutorial on surrogate gradient learning in spiking neural networks](https://github.com/fzenke/spytorch) :tada: So the tutorial demonstrated the use of surrogate gradient learning. It's a user friendly tutorial, but it's less object oriented, I think I understand most of the content.

__2020-09-19__: [Bootstrap Your Own Latent A New Approach to Self-Supervised Learning](https://arxiv.org/pdf/2006.07733.pdf) :tada: This is the first time I properly read BYOL. I like the idea of eliminating negative samples. However, what is not very clear to me is how come a metric-based few shot learning can efficiently learn a metric space through small number of samples. I found BYOL is much easier to be extended into other modalities as it doesn't need a creative contrastive learning task.

__2020-09-20__: None

__2020-09-21__: None

__2020-09-23__: None

__2020-09-24__: None

__2020-09-25__: [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/pdf/2006.09882.pdf) :tada: Pretty cool paper. Friendly with smaller number of batch sizes. There are two main tricks in this work: 1. online clustering part to supply the code for different views. 2. Multi-corp strategy.

__2020-09-26__: [Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/pdf/1912.01991.pdf)

__2020-09-27__: [Audio-Visual Instance Discrimination with Cross-Modal Agreement](https://arxiv.org/pdf/2004.12943.pdf)

__2020-09-28__: [Similarity of Neural Network Representations Revisited](http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf)

__2020-09-30__: [The Hardware Lottery](https://arxiv.org/pdf/2009.06489.pdf)

__2020-10-01__: [A Framework For Contrastive Self-Supervised Learning And Designing A New Approach](https://arxiv.org/pdf/2009.00104.pdf)

__2020-10-02__: [Evaluating Self-Supervised Pretraining Without Using Labels](https://arxiv.org/pdf/2009.07724.pdf)

__2020-10-03__: 

__2020-10-04__: 

__2020-10-05__: 

__2020-10-07__: 

__2020-10-08__: 

__2020-10-09__: 

__2020-10-10__: 

__2020-10-11__: 

__2020-10-12__: 

__2020-10-14__: 

__2020-10-15__: 

__2020-10-16__: 

__2020-10-17__: 

__2020-10-18__: 

__2020-10-19__: 

__2020-10-21__: 

__2020-10-22__: 

__2020-10-23__: 

__2020-10-24__: 

__2020-10-25__: 


## First Morning Paper Challenge

The first morning paper challenge is archived at [here](./first-challenge.md).

## Second Unsuccessful Paper Challenge Attempt

It's [here](./second-unsuccessful-attempt.md)

## Third Morning Paper Challenge

It's [here](./third-challenge.md)

## Fourth Morning Paper Challenge

It's [here](./fourth-challenge.md)

## Contacts

Yuhuang Hu  
Email: duguyue100@gmail.com
